{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an outline for your report to ease the amount of work required to create your report. Jupyter notebook supports markdown, and I recommend you to check out this [cheat sheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet). If you are not familiar with markdown.\n",
    "\n",
    "Before delivery, **remember to convert this file to PDF**. You can do it in two ways:\n",
    "1. Print the webpage (ctrl+P or cmd+P)\n",
    "2. Export with latex. This is somewhat more difficult, but you'll get somehwat of a \"prettier\" PDF. Go to File -> Download as -> PDF via LaTeX. You might have to install nbconvert and pandoc through conda; `conda install nbconvert pandoc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1a)\n",
    "\n",
    "![](screenshots/1a.png)\n",
    "\n",
    "## task 1b)\n",
    "\n",
    "![](screenshots/1b.png)\n",
    "\n",
    "## task 1c)\n",
    "\n",
    "![](screenshots/1c.png)\n",
    "\n",
    "## task 1d)\n",
    "\n",
    "![](screenshots/1d.png)\n",
    "\n",
    "## task 1e)\n",
    "\n",
    "![](screenshots/1e.png)\n",
    "\n",
    "## task 1f)\n",
    "\n",
    "![](screenshots/1f.png)\n",
    "\n",
    "## task 1g)\n",
    "\n",
    "![](screenshots/1g.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "\n",
    "### Task 2a)\n",
    "\n",
    "Below are a plot of the train and validation loss over the training period\n",
    "\n",
    "![](plots/task2_plot.png)\n",
    "\n",
    "### Task 2b)\n",
    "Our final accuracies were the following:\n",
    "\n",
    "|                     | Task 2 Model |\n",
    "|---------------------|:-------:|\n",
    "| Training accuracy   |  87,9%  |\n",
    "| Validation accuracy |  73,5%  |\n",
    "| Testing accuracy    |  73,4%  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3a)\n",
    "#### Model 1\n",
    "During the tuning process we tried tweaking the parameters mentioned below, as well as the activation function. We took notes to what every change meant to the model. Model 1 represents the parameter tuning that gave the best performance increase considering each parameter separately. As you ask for the report to be concise and not go too much into detail, we do not discuss all our findings in the tuning process.\n",
    "\n",
    "This model is altered from the original task 2 model in the following:\n",
    "* Data augmentation where 50% of the dataset is randomly mirrored and 10% has introduced contrast and saturation jitter.\n",
    "* Filter size is reduced to 3x3.\n",
    "* Batch normalization is introduced after every convolutional and fully connected layer.\n",
    "* As is shown in the table below, the number of filters is increased and the network achitecture is deeper.\n",
    "* Training details are unaltered and default, including optimizer, regularization, learning rate and batch size \n",
    "\n",
    "| Layer |  Layer Type  | Hidden units/filters | Activation f. |\n",
    "|:-----:|:------------:|:--------------------:|:-------------:|\n",
    "|   1   |    Conv2D    |          64          |      ReLU     |\n",
    "|   1   | BatchNorm2D  |          64          |        -      |\n",
    "|   1   |    Conv2D    |          64          |      ReLU     |\n",
    "|   1   | BatchNorm2D  |          64          |        -      |\n",
    "|   1   |   MaxPool2D  |          -           |       -       |\n",
    "|   2   |    Conv2D    |         128          |      ReLU     |\n",
    "|   2   | BatchNorm2D  |         128          |        -      |\n",
    "|   2   |    Conv2D    |         128          |      ReLU     |\n",
    "|   2   | BatchNorm2D  |         128          |        -      |\n",
    "|   2   |   MaxPool2D  |          -           |       -       |\n",
    "|   3   |    Conv2D    |         256          |      ReLU     |\n",
    "|   3   | BatchNorm2D  |         256          |        -      |\n",
    "|   3   |    Conv2D    |         256          |      ReLU     |\n",
    "|   3   | BatchNorm2D  |         256          |        -      |\n",
    "|   3   |   MaxPool2D  |          -           |       -       |\n",
    "|-------|--------------|----------------------|---------------|\n",
    "|       |    Flatten   |          -           |      ReLU     |\n",
    "|   4   |     FCNN     |          64          |      ReLU     |\n",
    "|   4   | BatchNorm1D  |          64          |        -      |\n",
    "|   5   |     FCNN     |          10          |     Softmax   |\n",
    "|   5   | BatchNorm1D  |          10          |        -      |\n",
    "\n",
    "#### Model 2\n",
    "For model 2, we wanted to make a good model without adding any filters. We also tried to avoid using Batch Normalization and Data Augmentation, as this was implemented in Model 1. After trying several things, we found that including dropout would improve our system substantially, from a validation accuracy of 73.5% to 78%. We also tried switching the activation function, with no substantial improvement. Not even using the Adam optimization technique lead to a better system. This is interesting, as Adam is known for performing better than SGD on most problems. SGD tends to work better on avoiding being stuck in local minimums, which might help to provide a solution for this behaviour.\n",
    "\n",
    "This model is altered from the original task 2 model in the following:\n",
    "* Filter size is reduced to 3x3.\n",
    "* Dropout with a probability of p=0.1\n",
    "* Training details are unaltered and default, including optimizer, regularization, learning rate and batch size \n",
    "\n",
    "| Layer |  Layer Type  | Hidden units/filters | Activation f. |\n",
    "|:-----:|:------------:|:--------------------:|:-------------:|\n",
    "|   1   |    Conv2D    |          64          |      ReLU     |\n",
    "|   1   |   MaxPool2D  |          -           |       -       |\n",
    "|   1   |Dropout(p = 0.1) |          -        |       -       |\n",
    "|   2   |    Conv2D    |         128          |      ReLU     |\n",
    "|   2   |   MaxPool2D  |          -           |       -       |\n",
    "|   2   |Dropout(p = 0.1) |          -        |       -       |\n",
    "|   3   |    Conv2D    |         256          |      ReLU     |\n",
    "|   3   |   MaxPool2D  |          -           |       -       |\n",
    "|   3   |Dropout(p = 0.1) |          -        |       -       |\n",
    "|-------|--------------|----------------------|---------------|\n",
    "|       |    Flatten   |          -           |      ReLU     |\n",
    "|   4   |     FCNN     |          64          |      ReLU     |\n",
    "|   5   |     FCNN     |          10          |     Softmax   |\n",
    "\n",
    "### Task 3b)\n",
    "|                     | Model 1 | Model 2 |\n",
    "|---------------------|:-------:|:-------:|\n",
    "| Training accuracy   |  83,7%  |  93,2%  |\n",
    "| Validation accuracy |  79,8%  |  78,0%  |\n",
    "| Testing accuracy    |  79,5%  |  77,7%  |\n",
    "\n",
    "#### Model 1 performance\n",
    "![](plots/task3_model1_plot.png)\n",
    "\n",
    "### Task 3c)\n",
    "We found that adjusting the network architecture and filter number (and filter size to some degree) had a great impact on model performance. This is likely because there are so many hyperparameters in a model of this size, and finding the optimal tuning is a very work-intensive process. As such, finding the optimal network architecture first try for instance, is impossible. \n",
    "\n",
    "Also, implementing dropout proved to have substantial impact on model performance. Dropout is a method used to prevent overfitting. Interestingly, when using dropout on our model, the training accuracy is increasing more than the validation accuracy, implying overfitting. Also, we saw a slight worse performance when including dropout to Model 1. \n",
    "\n",
    "Data augmentation also had a great effect once the model was made deeper with more layers and filters, but by itself it did not do much. The data augmentation method we implemented in model 1 was what we found to be optimal when only data augmentation was used to improve on the base model from task 2. In fact, model 2 differs from model 1 only in the degree of data augmentation. It turns out that when the model is deeper, more generalized and complex (as is the case for model 2), it responds better to a more diverse dataset. \n",
    "\n",
    "Batch normalization also did not affect model performance, however it did allow for a much faster learning rate, especially in the deeper models. This is likely because of an inate internal covariate shift, which is typical when models grow more complex. Batch normalization was however crucial when adding more layers and growing our model depth. As we promptly realized, adding more layers actually had a negative effect on performance until we also decided to include batch normalization. With this combination, the model excelled!\n",
    "\n",
    "As mentioned above, we also tried swapping the ReLU activation function for others, including LeakyRELU, Hardswish, ELU, PReLU, ReLU6, RReLU, SELU, CELU and GELU. None of these showed any improved performance, however many came very close. With further testing, some activation functions could potentially show promise for more complex models.\n",
    "\n",
    "TODO(?): Try to implement strided convolutions instead of pooling to have something to write about. I intentionally skipped this method because I thought it would have no positive effect on performance.\n",
    "\n",
    "### Task 3d)\n",
    "The plot below shows how the modeled improved by introducing batch normalization. As one can see, the validation accuracies and the losses are improved substantially. Even more prominent, the learning speed is considerably improved.\n",
    "\n",
    "![](plots/task3d_plot.png)\n",
    "\n",
    "### Task 3e)\n",
    "After changing the data autmentation of model 1 to **20%** of the data set being introduced contrast and saturation jitter, in comparison to the prevous 10%, the accuracies was improved substantially. The final values, as well as a plot of validation accuracy, can be seen below.\n",
    "\n",
    "|                     | Model 1 | Improved Model |\n",
    "|---------------------|:-------:|:-------:|\n",
    "| Training accuracy   |  83,7%  |  87,2%  |\n",
    "| Validation accuracy |  79,8%  |  83,2%  |\n",
    "| Testing accuracy    |  79,5%  |  82,7%  |\n",
    "\n",
    "![](plots/task3e.png)\n",
    "\n",
    "We found this quite large increase in accuracy very interesting. After some research, we concluded that when the model is deeper, more generalized and complex, it responds better to a more diverse dataset. This might explain why our quite complex model responds very well to data augmentation. \n",
    "\n",
    "\n",
    "\n",
    "### Task 3f)\n",
    "Yes, overfitting is clearly present in our model, as well as in all other models we encountered during tuning. It can be observed by the fact that the training accuracy is always significantly better than both the validation- and testing accuracy. Interestingly, we found that for every act of data augmentation, the training accuracy was reduced while the validation and test accuracy was increased. This is of course desirable behaviour, and is likely because the dataset is generalized in a way. Data augmentation is essentially a way of increasing model performance while reducing overfitting.\n",
    "\n",
    "Overfitting is however not as apparent as was the case in one of our earlier FCNNs. This may be because a 3 channel 32x32 pixel image gives rise to features which are much harder for a model to \"memorize\", compared to much simpler FCNNs. The vastly greater number of model parameters likely also has an effect in this regard. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4a)\n",
    "![](plots/task4a_plot.png)\n",
    "As can be seen in the above figure, we achieved a training accuracy of 89,9% by utilizing transfer learning with Resnet18."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4b)\n",
    "![](plots/task4b.png)\n",
    "From the filter activations above, it can be observed how the differing filters affect how horizontal and vertical features are detected to differing degrees. Filter 14, 26 and 49 are better suited to detect sharp vertical (filter 14), horizontal (filter 26) and diagonal (filter 49) edges, which is apparent in the distinct zebra lines for their corresponding activations. Filters 32 and 52 however, detect different color variations than simply greyscale, and are in addition less suited to detect edges because of their \"round\" shape.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4c)\n",
    "![](plots/task4c.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
