{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4 Report - Group 200 - Jostein Lysberg and Ludvig LÃ¸ite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1a)\n",
    "The Intersection over Union is simply a measure of the equality of two bounding boxes, given their respective overlap. It is often used to estimate how accurate a bounding box prediction is, compared to the ground truth. For two bounding boxes, we can calculate this value by dividing the area of overlap with the area of union. \n",
    "\n",
    "![](report_images/drawing_1a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1b)\n",
    "A true positive is defined as a positive prediction on a class which is positive in reality. This prediction is correct. A false positive is also a positive prediction, but in this case the class is negative in reality. This prediction is therefore not correct.\n",
    "\n",
    "![](http://www.sciweavers.org/tex2img.php?eq=Precision%20%3D%20%20%5Cfrac%7BTP%7D%7BTP%20%2B%20FP%7D&bc=White&fc=Black&im=jpg&fs=12&ff=arev&edit=0) \n",
    "\n",
    "![](report_images/recall.png)\n",
    "\n",
    "\n",
    "## task 1c)\n",
    "\n",
    "mAP for class 1:\n",
    "\n",
    "![](report_images/map1.png)\n",
    "\n",
    "mAP for class 2:\n",
    "![](report_images/map2.png)\n",
    "\n",
    "Total mAP:\n",
    "\n",
    "![](report_images/totalmap.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "\n",
    "### Task 2f)\n",
    "\n",
    "Below is the plot of our final precision-recall curve\n",
    "\n",
    "![](task2/precision_recall_curve_task2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3a)\n",
    "The filtering operation of removing duplicate predictions pointing to the same object is called non-maximum suppression. This step is performed at the very end of the model, after the convolutional network has produced a collection of bounding boxes and scores for the presence of object class instances in those boxes.\n",
    "\n",
    "### Task 3b)\n",
    "False. In SSD, Small objects are detected at the earlier, higher-resolution feature maps. This is because the bounding boxes come in fixed sizes, and will therefore cover larger proportions of the feature maps as the deeper layers shrink in resolution.\n",
    "\n",
    "### Task 3c)\n",
    "The reason for utilizing several different bounding boxes with distinct aspect ratios is that all categories to be detected do not have arbitrary shapes and sizes. To some extent, all objects come in inherently predictable shapes, which is reflected in the wide range of bounding boxes in SSD.\n",
    "\n",
    "### Task 3d)\n",
    "One major distinction between SSD and YOLO is the addition of several convolutional feature layers to the end of a base network to predict a shape offset relative to the default box coordinates, as opposed to YOLO's use of a fully connected layer for this step. This shape offset is in turn associated with the confidences of all default boxes with their different scales and aspect ratios.\n",
    "\n",
    "### Task 3e)\n",
    "Assuming k=6 different default boxes and a feature map with resolution 38x38, this particular feature map will consist of 8664 anchor boxes.\n",
    "\n",
    "![](report_images/anchor1.png)\n",
    "\n",
    "### Task 3f)\n",
    "Assuming k=6 different bounding boxes, the entire network would consist of 11640 anchor boxes.\n",
    "\n",
    "![equation](report_images/anchor2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| Model from task  |  Number of iterations  | mAP |\n",
    "|:-----:|:------------:|:--------------------:|\n",
    "|   4b   |    6000     |          75.8 %          |      \n",
    "|   4c   |    10000    |          87.6 %          |        \n",
    "|   4d   |    14500    |          90.3 %          |    \n",
    "|   4f   |    5000     |          47.1 %          |\n",
    "\n",
    "## Task 4b)\n",
    "\n",
    "Below, you can see the plot of \"total_loss\", taken from tensorboard. We were able to reach a mAP value of 75.8%.\n",
    "\n",
    "![](report_images/4b_total_loss.png)\n",
    "\n",
    "## Task 4c)\n",
    "\n",
    "We did several changes to improve our model:\n",
    "\n",
    "1. We added more layers to deepen our network, by adding a convolution layer to each of the 6 blocks. We ended up with around 3.9 million parameters.\n",
    "2. We added Batch Normalization after each convolution layer except the first and the last in each of the 6 blocks. \n",
    "3. We included the RandomSampleCrop() transform\n",
    "\n",
    "Our final mAP value was 87.6%.\n",
    "\n",
    "## Task 4d)\n",
    "\n",
    "We were able to reach a mAP value of 90.3% after running 14500 iterations, as can be seen from the plot below. This was achieved by adjusting the threshold, and by lowering the minimum box sizes.\n",
    "\n",
    "![](report_images/map_4d.png)\n",
    "\n",
    "\n",
    "## Task 4e)\n",
    "The following pictures show our classified images using a score threshold of 0.3. With this low threshold, all the detections should not be taken as the truth, at least not for critical applications. We used this low threshold to get a clear understanding of model performance. Our trained model was able to detect most digits, but was worse at detecting those contained within small bounding boxes. Most of the digits that were significantly smaller than the rest, had a score of less than 0.5. Some were even lower than our score threshold of 0.3, and were not classified at all. This is a known weakness of SSD, and is likely a result of its method for handling different object scales. The fact that prediction is performed in the feature maps of several different layers of a single network, as opposed to processing the image at different sizes and combining the result afterwards, might affect the small objects that consistently rely on being classified in the earlier, high-resolution convolutional layers. As mentioned in the paper by Liu, Anguelov, Erhan, Szegedy, Reed, Fu and Berg, this problem is reduced to some degree by introducing random sample cropping, as we have shown in task 4c and 4d. We also saw that the digit one consistently scored worse than the rest of the digits during training. In the final model, however, one is classified approximately equal to the other digits. We think the model's bad performance on classifying this particular digit might be explained by the fact that ones generally cover small bounding boxes, which has proven to be a weakness of SSD. Another explaination could be the fact that parts of other digits are very similar to one. For example, most occurences of seven, and many poorly written occurences of nines or fours could prove to be a problem. \n",
    "\n",
    "\n",
    "|   |       |   |  \n",
    "|:-------------------------:|:-------------------------:|:-------------------------:|\n",
    "|![](SSD/demo/mnist/result/0.png)   |  ![](SSD/demo/mnist/result/1.png)  | ![](SSD/demo/mnist/result/2.png) |\n",
    "|![](SSD/demo/mnist/result/3.png)   |  ![](SSD/demo/mnist/result/4.png)  | ![](SSD/demo/mnist/result/5.png) |\n",
    "|![](SSD/demo/mnist/result/6.png)   |  ![](SSD/demo/mnist/result/7.png)  | ![](SSD/demo/mnist/result/8.png) |\n",
    "|![](SSD/demo/mnist/result/9.png)   |  ![](SSD/demo/mnist/result/10.png)  | ![](SSD/demo/mnist/result/11.png) |\n",
    "|![](SSD/demo/mnist/result/12.png)   |  ![](SSD/demo/mnist/result/13.png)  | ![](SSD/demo/mnist/result/14.png) |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Task 4f)\n",
    "\n",
    "Our final mAP was 47.1%. The plot of \"total loss\" and the classified images can be seen below. We used a threshold of 0.3 to get further insight into classification of our model, than by using the default of 0.7.\n",
    "\n",
    "![](report_images/4f_total_loss.png)\n",
    "\n",
    "\n",
    "|   |       |\n",
    "|:-------------------------:|:-------------------------:|\n",
    "|![](SSD/demo/voc/result/000342.png)   |  ![](SSD/demo/voc/result/000542.png) |\n",
    "|![](SSD/demo/voc/result/003123.png)   |  ![](SSD/demo/voc/result/004101.png) |\n",
    "|![](SSD/demo/voc/result/008591.png)   |   |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
