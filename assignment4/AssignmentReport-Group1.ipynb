{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an outline for your report to ease the amount of work required to create your report. Jupyter notebook supports markdown, and I recommend you to check out this [cheat sheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet). If you are not familiar with markdown.\n",
    "\n",
    "Before delivery, **remember to convert this file to PDF**. You can do it in two ways:\n",
    "1. Print the webpage (ctrl+P or cmd+P)\n",
    "2. Export with latex. This is somewhat more difficult, but you'll get somehwat of a \"prettier\" PDF. Go to File -> Download as -> PDF via LaTeX. You might have to install nbconvert and pandoc through conda; `conda install nbconvert pandoc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1a)\n",
    "The Intersection over Union is simply a measure of the equality of two bounding boxes, given their respective overlap. It is often used to estimate how accurate a bounding box prediction is, compared to the ground truth. For two bounding boces, we can calculate this value by dividing the area of overlap with the area of union. \n",
    "\n",
    "TODO: Add illustration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1b)\n",
    "A true positive is defined as a positive prediction on a class which is positive in reality. This prediction is correct. A false positive is also a positive prediction, but in this case the class is negative in reality. This prediction is therefore not correct.\n",
    "\n",
    "![equation](http://www.sciweavers.org/tex2img.php?eq=Precision%20%3D%20%20%5Cfrac%7BTP%7D%7BTP%20%2B%20FP%7D&bc=White&fc=Black&im=jpg&fs=12&ff=arev&edit=0)  \n",
    "\n",
    "![equation](http://www.sciweavers.org/tex2img.php?eq=Recall%20%3D%20%20%5Cfrac%7BTP%7D%7BTP%20%2B%20FN%7D%20&bc=White&fc=Black&im=jpg&fs=12&ff=arev&edit=0)\n",
    "\n",
    "## task 1c)\n",
    "mAP for class 1:\n",
    "![equation](http://www.sciweavers.org/tex2img.php?eq=mAP%3D%20%5Cfrac%7B1%7D%7B11%7D%20%2A%288%2A1.0%2B3%2A0.7%2B0%2A0.2%29%20%5Capprox%200.92&bc=White&fc=Black&im=jpg&fs=12&ff=arev&edit=0)\n",
    "\n",
    "mAP for class 2:\n",
    "![equation](http://www.sciweavers.org/tex2img.php?eq=mAP%3D%20%5Cfrac%7B1%7D%7B11%7D%20%2A%285%2A1.0%2B1%2A0.8%2B2%2A0.6%2B3%2A0.5%29%20%5Capprox%200.77&bc=White&fc=Black&im=jpg&fs=12&ff=arev&edit=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "\n",
    "### Task 2f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3a)\n",
    "The filtering operation of removing duplicate predictions pointing to the same object is called non-maximum suppression. \n",
    "\n",
    "### Task 3b)\n",
    "False. In SSD, Small objects are detected at the earlier, higher-resolution feature maps.\n",
    "\n",
    "### Task 3c)\n",
    "The reason for utilizing several different bounding boxes with distinct aspect ratios is that all categories to be detected do not have arbitrary shapes and sizes. To some extent, all objects come in inherently predictable shapes, which is reflected in the wide range of bounding boxes in SSD.\n",
    "\n",
    "### Task 3d)\n",
    "One major distinction between SSD and YOLO is the addition of several convolutional feature layers to the end of a base network to predict a shape offset relative to the default box coordinates, as opposed to YOLO's use of a fully connected layer for this step. This shape offset is in turn associated with the confidences of all default boxes with their different scales and aspect ratios.\n",
    "\n",
    "### Task 3e)\n",
    "Assuming only one class (c = 1), 6 different default boxes and a feature map with resolution 38x38, this particular feature map will consist of 43320 anchor boxes, or filters.\n",
    "\n",
    "![equation](http://www.sciweavers.org/tex2img.php?eq=filters%20%3D%20%28c%20%2B%204%29%2Ak%2Am%2An%20%3D%205%2A6%2A38%2A38%20%3D%2043320%0A&bc=White&fc=Black&im=jpg&fs=12&ff=arev&edit=0)\n",
    "\n",
    "### Task 3f)\n",
    "Again, assuming only one class (c = 1), the entire network would consist of 58200 anchor boxes.\n",
    "\n",
    "![equation](http://www.sciweavers.org/tex2img.php?eq=filters%20%3D%20%28c%20%2B%204%29%2Ak%2A%2838%5E%7B2%7D%2B19%5E%7B2%7D%2B10%5E%7B2%7D%2B5%5E%7B2%7D%2B3%5E%7B2%7D%2B1%5E%7B2%7D%29%3D58200%0A&bc=White&fc=Black&im=jpg&fs=12&ff=arev&edit=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| Model from task  |  Number of iterations  | mAP |\n",
    "|:-----:|:------------:|:--------------------:|\n",
    "|   4b   |    6000     |          75.8 %          |      \n",
    "|   4c   |    10000    |          87.6 %          |        \n",
    "|   4d   |    14500    |          90.3 %          |    \n",
    "|   4f   |    5000     |          47.1 %          |\n",
    "\n",
    "## Task 4b)\n",
    "\n",
    "Below, you can see the plot of \"total_loss\", taken from tensorboard. We were able to reach a mAP value of 75.8%.\n",
    "\n",
    "## Task 4c)\n",
    "\n",
    "We did several changes to improve our model:\n",
    "\n",
    "1. We added more layers to deepen our network, by adding a convolution layer to each of the 6 blocks. We ended up with around 3.9 million parameters.\n",
    "2. We added Batch Normalization after each convolution layer except the first and the last in each of the 6 blocks. \n",
    "3. We included the RandomSampleCrop() transform\n",
    "\n",
    "Our final mAP value was 87.6%.\n",
    "\n",
    "## Task 4d)\n",
    "\n",
    "We were able to reach a mAP value of 90.3% after running 14500 iterations, as can be seen from the plot below. This was achieved by adjucting the threshold, and by adjucting the minimum box sizes.\n",
    "\n",
    "![](report_images/map_4d)\n",
    "\n",
    "\n",
    "## Task 4e)\n",
    "The following pictures show our classified images using a score threshold of 0.3. With this low threshold, all the detections should not be taken as the truth, at least not for critical applications. We used this low threshold to see the more of our models work. Our trained model were able to detect most numbers, but were worse at detecting small numbers. Most of the numbers that were significanlty smaller than the rest, had a score of below 0.5, some of them not even being classified with our score threshold of 0.3. We also saw that the digit 1 consistently scored worse than the rest of the digits during the training. In the final model, however, 1 is classified approximately equal to the other digits. We think the model's bad performance on classifying 1 might be explained by the fact that 1's covers a small area, which our model is bad at detecting, and the fact that parts of other digits are very similar to 1. For example, the bottom part of 7 is very similar to 1. \n",
    "\n",
    "\n",
    "## Task 4f)\n",
    "\n",
    "Our final mAP was 47.1%. The plot of \"total loss\" and the classified images can be seen below. We used a threshold of 0.3 to get further insight into classification of our model, than by using the default of 0.7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
