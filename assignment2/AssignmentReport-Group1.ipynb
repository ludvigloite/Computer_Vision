{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 Report - Group 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1a)\n",
    "\n",
    "![](1a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1b)\n",
    "\n",
    "![](1b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2c)\n",
    "![](task2c_train_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2d)\n",
    "\n",
    "Our model is comprised of one input layer with 785 nodes (taking bias into account), one hidden layer with 64 nodes, and one output layer with 10 nodes. This gives rise to the following amount of parameters:\n",
    "![equation](http://www.sciweavers.org/tex2img.php?eq=785%2A64%2B64%2A10%3D50880&bc=White&fc=Black&im=jpg&fs=12&ff=arev&edit=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](task3a_train_loss.png)\n",
    "In the above figure, the orange graph represent an improved model where the input weights of every node are initialized to a normal distribution with a standard deviation inversely proportional to the number of nodes in the preceding layer. The blue graph represents a model with weights of a uniform distribution. In both models, the weights are initialized to a mean of 0. In the model with improved weights, we got a significant increase in performance. The time usage was almost identical.\n",
    "\n",
    "![](task3b_train_loss.png)\n",
    "This time, we improved our model by implementing a new sigmoid function, as described in _Efficient Backprop, section 4.4_. Both models have the improved weights. The improved model is represented by the orange graph in the above figure. The convergence time of the improved model is almost half of the convergence time with the old sigmoid function. As well as being alot quicker, the improved model convergence to a higher validation accuracy.\n",
    "\n",
    "![](task3c_train_loss.png)\n",
    "By implementing momentum, as described in _Efficient Backprop, section 4.7_, we saw a large improvement in the convergence speed. The validation accuracy was nearly equal. \n",
    "\n",
    "On a general basis, when the convergence speed is reduced, the generalization is also improved. The longer the model runs, the more specified it gets to the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4a, 4b)\n",
    "![](task4ab_accuracy.png)\n",
    "As is apparent in the above figure, the number of hidden units affect both model performance and convergence rate. By halving the amount of hidden units from 64 to 32, the validation accuracy is reduced by about 2 percentage points, while increasing the rate of convergence by about 50 percent. The same tendency can be seen by doubling the amount of hidden units from 64 to 128, where the validation accuracy is increased by about 1 percentage point. In this case, however, the convergence time is more than doubled. In conclusion, the amount of hidden units in our model is a trade off between performance and convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4d)\n",
    "In task 3, the parameter count is equal to that of task 2, which was calculated to be 50880. With two hidden layers of 60 nodes, we end up with the following parameter count:\n",
    "\n",
    "![equation](https://bit.ly/3pB5zEI)\n",
    "\n",
    "![](task4d.png)\n",
    "This new model with two hidden layers converge to approximately the same training and validation accuracies as the model with only one hidden layer, but with the same number of parameters. One thing to notice is that the model with only one hidden layer converges around 25% faster than the model with two hidden layers. Also, one can observe that the accuracies of the model with two hidden layers are fluctuating more than the accuracies of the model with only one hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4e)\n",
    "![](task4de.png)\n",
    "We observed some interesting things when including plots for a model with 10 hidden layers and a total of 88700 parameters. The most obvious observation is that the model use more time. The model with 10 hidden layers used around 10 times longer to converge than the model with only one hidden layer. The reason for this is the added complexity with the added layers.\n",
    "\n",
    "One can also observe that the training and validation accuracies are lower than the models with fewer hidden layers. We think the model using 10 hidden layers is too complex for the task it is assigned to do. 10 layers with 64 nodes each to analyse a image with 28x28 pixels, where most of them are black, might prove inefficient and inadequate. When dealing with oversized neural networks, overfitting might be a problem. With this large amount of parameters, one parameter try to predict an extremely spesific feature of the input layer. The neural network might find and connect features that does not in any way correspond to any number.\n",
    "\n",
    "The accuracies of the model with 10 hidden layers are fluctuating alot more than the accuracies of the other models. This might also be a symptom of overfitting. The model is not able to get a consistent accuracy. Another observation linked to the fluctiation of the validation accuracy is that the accuracy is hardly improving in the last half of the training steps. The inconsistent accuracy have spikes that keep the early stopping from kicking in."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
